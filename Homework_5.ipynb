{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ADM - Homework 05\n",
    "\n",
    "## Visit the Wikipedia hyperlinks graph!\n",
    "\n",
    "#### Group 28 - Hafiz Muhammad, Marco Minici, Fran√ßois Chassaing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the custom functions coded to solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw05_FUNCTIONS import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing all the necessary python packages to perform the analysis requested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "from os.path import isfile\n",
    "from statistics import median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ1\n",
    "\n",
    "**Build the graph G=(V, E), where V is the set of articles and E the hyperlinks among them, and provide its basic information:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. If it is direct or not?**\n",
    "\n",
    "To answer this question we have to add edges and nodes to the networkx librarary. There is function called **nx.is_directed(G)** whether the graph is directed or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph() # notice directed graph\n",
    "f = open(\"data/wiki-topcats-page-names.txt\", \"r\")\n",
    "for row in f:\n",
    "    lrow = row.split()\n",
    "    name = ' '.join(lrow[1:])\n",
    "    G.add_node(int(lrow[0]))\n",
    "    G.node[int(lrow[0])][\"name\"] = name\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding nodes we have to add edges for these lets use **wiki-topcats-reduced.txt** file so that we have insights about the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flink = open(\"data/wiki-topcats-reduced.txt\", \"r\")\n",
    "for i,row in enumerate(flink):\n",
    "    lrow = row.split()\n",
    "    G.add_edge(int(lrow[0]), int(lrow[1])) #adding edges\n",
    "flink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed(G) # this function will tell us whether it is directed or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tells us that graph we are trying to work with is directed because edge meaning <node1,node2> in node1 wikipedia article got a link to the node2 wikipedia article.* so this answer the question whether the graph is directed or not.* **The graph is directed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes? The number of edges? \n",
    "for this we are going to use simple functions like **number_of_nodes** and **number_of_edges()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1791489"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes are **1791489** and number of edges are **2174451**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average node degree. Is the graph dense?\n",
    "\n",
    "To answer this we will use *density()** and degree function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9531267007500466\n"
     ]
    }
   ],
   "source": [
    "degreeM = 0\n",
    "for node in G.node:\n",
    "    degreeM += G.degree(node)\n",
    "print(degreeM/G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.242105726496763e-07"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to answer above question the average node degree is *2.4275348606661833* and density is *1.3550383037263901e-06*. which says that the graph is not much dense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move to reasearch question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2:\n",
    "Given a category C_0 = \\{article_1, article_2, \\dots \\} as input we want to rank all of the nodes in V according to the given criteria discussed in given question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declaring some preliminary variables that will be used multiple times sequently in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the folder path where all the data are stored\n",
    "dataFolder = \"./data/\"\n",
    "#base filename for all wiki data\n",
    "basefilename = \"wiki-topcats-\"\n",
    "#file extension of wiki data\n",
    "ext = \".txt\"\n",
    "#name of all wiki data files\n",
    "CATEGORIES = \"categories\"\n",
    "REDUCED_GRAPH = \"reduced\"\n",
    "PAGE_NAMES = \"page-names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the file containing the *categories*.\n",
    "\n",
    "This file contains a list of categories. Each category is a wikipedia articles collection, where each article belongs to the specific category.\n",
    "\n",
    "The operation performed are:\n",
    "\n",
    "    - File reading with pandas package.\n",
    "    - Cleaning the \"category\" column deleting the redundant \"Category:\" string, let the value be only the category name.\n",
    "    - Retain only the categories which have more than 3500 articles inside\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's open the wiki_categories.txt file and delete all categories\n",
    "#with less than 3500 articles\n",
    "#\n",
    "#read the category file\n",
    "categories = pd.read_csv(dataFolder+basefilename+CATEGORIES+ext, sep = \";\", header = None , names = [\"Category\",\"List_of_articles\"])\n",
    "#clean the category column\n",
    "#Before: \"Category: category_name\"\n",
    "#After: \"category_name\"\n",
    "categories[\"Category\"] = categories[\"Category\"].apply(lambda x:x.split(\":\")[1])\n",
    "#defining a function that delete all categories with less than 3500 articles\n",
    "ARTICLES_THRESHOLD = 3500\n",
    "#Scanning each value of list_of_articles, if the number of articles goes beyond the threshold then retain it\n",
    "#otherwise return an empty string\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: x if len(x.strip().split(\" \")) >= ARTICLES_THRESHOLD else \"\")\n",
    "#delete all rows with empty string as \"list_of_articles\"\n",
    "categories = categories[categories[\"List_of_articles\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the graph representing the wikipedia articles network must be built, before creating the mentioned data structure it is needed to check if the nodes contained in the *category* file and the nodes in the *graph* are the same. This is a necessary condition to perform a consistent analysis.\n",
    "\n",
    "The consistency check is performed as follows:\n",
    "\n",
    "    - Preliminary reading all the edges of the graph using the pandas library.\n",
    "    - Create the set of nodes which compare at least in one of the edges, either as the starting node or as the final node.\n",
    "    - Create the set of nodes which compare at least in one of the categories, an integer transformation from string is necessary, this allows to make a comparison with the nodes of the graph.\n",
    "    - Perform the intersection between the set of nodes found in the graph and the set of nodes found in the categories file. \n",
    "    \n",
    "The final set produced will be sequently used to normalize the categories and the graph file, retaining only the nodes present in both.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of set_node_1:= 428957\n",
      "size of set_node_2:= 352518\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_categories_nodes:= 546237\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_of_nodes:= 461193\n"
     ]
    }
   ],
   "source": [
    "#Now it is needed to build the final graph, but it must be checked that the nodes into the reduced graph and \n",
    "#the nodes into the categories files are the same.\n",
    "#The set of the nodes into the categories must be built.\n",
    "#The set of nodes into the reduced graph must be built.\n",
    "#the intersection between these two sets must be computed.\n",
    "#only the edges involving nodes of the intersection set must be added to the final graph\n",
    "\n",
    "#let's open the reduced graph file and create the set of nodes \n",
    "reduced_graph = pd.read_csv(dataFolder+basefilename+REDUCED_GRAPH+ext, sep = \"\\t\", header = None, names = [\"Node_1\",\"Node_2\"])\n",
    "#create the set of the first column\n",
    "set_node_1 = set(reduced_graph[\"Node_1\"].values.tolist())\n",
    "#create the set of the second column\n",
    "set_node_2 = set(reduced_graph[\"Node_2\"].values.tolist())\n",
    "#create the set of the nodes into the reduced graph through the union operation of the two previous created sets\n",
    "set_reduced_graph_nodes = set.union(set_node_1,set_node_2)\n",
    "#print the size for debugging\n",
    "print(\"size of set_node_1:= \"+str(len(set_node_1)))\n",
    "print(\"size of set_node_2:= \"+str(len(set_node_2)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "#The two previous sets are not useful anymore therefore they are deleted from the main memory\n",
    "del set_node_1\n",
    "del set_node_2\n",
    "\n",
    "#Let's create the set of nodes into the categories\n",
    "#create initially an empty set\n",
    "set_categories_nodes = set()\n",
    "#in order to perform the intersection function the nodes must be represented in the same format\n",
    "#since the nodes into the @reduced_graph dataframe are integer then the \"int\" type is chosen\n",
    "categories[\"List_of_articles\"].apply(lambda x: set_categories_nodes.update(set(map(int, x.strip().split(\" \")))))\n",
    "\n",
    "#compute the final set of nodes\n",
    "set_of_nodes = set.intersection(set_categories_nodes, set_reduced_graph_nodes)\n",
    "#print the size for debugging\n",
    "print(\"size of set_categories_nodes:= \"+str(len(set_categories_nodes)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "print(\"size of set_of_nodes:= \"+str(len(set_of_nodes)))\n",
    "#once the final set is computed the other sets can be deleted to free the memory\n",
    "del set_categories_nodes\n",
    "del set_reduced_graph_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instatiate the graph representing the wiki articles network.\n",
    "\n",
    "It has been chosen a *networx.DiGraph* class because a general edge $<Node_1, Node_2>$ represents [the presence into the $Node_1$ wiki article of a link which points to the wiki article $Node_2$.](https://snap.stanford.edu/data/wiki-topcats.html)\n",
    "\n",
    "This kind of relationship must be encoded as a directed graph.\n",
    "\n",
    "Before adding the edge a filtering is applied, checking if both nodes of the edges are present in the set of nodes previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          None\n",
       "1          None\n",
       "2          None\n",
       "3          None\n",
       "4          None\n",
       "5          None\n",
       "6          None\n",
       "7          None\n",
       "8          None\n",
       "9          None\n",
       "10         None\n",
       "11         None\n",
       "12         None\n",
       "13         None\n",
       "14         None\n",
       "15         None\n",
       "16         None\n",
       "17         None\n",
       "18         None\n",
       "19         None\n",
       "20         None\n",
       "21         None\n",
       "22         None\n",
       "23         None\n",
       "24         None\n",
       "25         None\n",
       "26         None\n",
       "27         None\n",
       "28         None\n",
       "29         None\n",
       "           ... \n",
       "2645217    None\n",
       "2645218    None\n",
       "2645219    None\n",
       "2645220    None\n",
       "2645221    None\n",
       "2645222    None\n",
       "2645223    None\n",
       "2645224    None\n",
       "2645225    None\n",
       "2645226    None\n",
       "2645227    None\n",
       "2645228    None\n",
       "2645229    None\n",
       "2645230    None\n",
       "2645231    None\n",
       "2645232    None\n",
       "2645233    None\n",
       "2645234    None\n",
       "2645235    None\n",
       "2645236    None\n",
       "2645237    None\n",
       "2645238    None\n",
       "2645239    None\n",
       "2645240    None\n",
       "2645241    None\n",
       "2645242    None\n",
       "2645243    None\n",
       "2645244    None\n",
       "2645245    None\n",
       "2645246    None\n",
       "Length: 2645247, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now it is possible to read line by line all the edges of the reduced graph\n",
    "#and adding it to the final graph only if they belong to the @set_of_nodes computed\n",
    "final_graph = nx.DiGraph()\n",
    "\n",
    "#build the graph\n",
    "reduced_graph.apply(lambda edge: final_graph.add_edge(edge[0],edge[1]) if filterEdges(edge,set_of_nodes) else \"\" ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CHUNK INTO WHICH THE AVERAGE NODE DEGREE AND THE DENSITY OF THE GRAPH MUST BE COMPUTED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same kind of filtering operation is performed on the categories file deleting all the nodes not belonging to the intersection set previously computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the category dataframe must involve only \"good nodes\" too\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: filter_nodes_in_categories(x,set_of_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To speed-up the analysis, the distance between each category is computed offline and stored in a file(in this case, it is named *category_distances.npy*)representing a (C_n,C_n)-shaped matrix where C_n is equal to the number of categories contained in the dataset.\n",
    "\n",
    "If the file isn't found on the filesystem, the matrix is computed in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_distances = retrieveCategoryDistances(filename = \"category_distances.npy\", final_graph = final_graph ,categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify and to speed-up the analysis, instead to have the categories as a *pandas.DataFrame* it is preferable to code them as a *python built-in dictionary*, where the keys are the category name and the values are the list of wikipedia articles belonging to that category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a dictionary that has as keys the categories and as values the list of articles\n",
    "#that belongs to that category\n",
    "category_dictionary = {}\n",
    "#for each category\n",
    "for i in range(categories.shape[0]):\n",
    "    #assign to that category the list of articles\n",
    "    category_dictionary[i] = list(map(int, categories.iloc[i]['List_of_articles'].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of ranking computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is, given an input category( in this example, it is $C_0$ ), to compute the block ranking vector where the blocks are represented by the categories.\n",
    "\n",
    "The first category of the rank, $C_0$, always corresponds to the input category. The order of the remaining categories is given by:\n",
    "\n",
    "$$distance(C_0,C_i) = median(ShortestPath(C_0, C_i))$$\n",
    "\n",
    "The lower is the distance from $C_0$, the higher is the $C_i$ position in the rank. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shortest path computation procedure explained:\n",
    "\n",
    "Given an input category $C_0$ and another category $C_i$, it exists at most one shortest path that goes from the category $C_0$ to each of the nodes $C_i$.\n",
    "\n",
    "Therefore, only for the purpose of computing the block ranking, it is added a *fake node* which has an in-degree equal to 0 and has as many out-edges as nodes contained in the $C_0$ category.\n",
    "\n",
    "The image that follows will explain better the mentioned situation.\n",
    "\n",
    "The *green* node represents the fake node, the *yellow* ones belongs to the input category and the *red* represents the rest of the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the *fake node* and the related edges are added, the distance computation can be performed.\n",
    "\n",
    "It is known that the graph has the following properties:\n",
    "\n",
    "    - It is directed.\n",
    "    - It isn't weighted.\n",
    "    \n",
    "Given these two properties to compute the shortest paths from the *fake_node* to all the nodes in the graph, it is preferrable to use the [Breadth-First-Search](https://en.wikipedia.org/wiki/Breadth-first_search) algorithm. \n",
    "The neighbours of the *root* node are marked as 1-distanced, each time a new *level* is explored the counter variable is increased, so the neighbours of the neighbours of the *root* node will be marked as 2-distanced and so on and so forth.\n",
    "\n",
    "The BFS will be performed only one time on the *fake_node* and so the time-complexity of the algorithm to compute the distance between one input category and all the others in the world is equal to the BFS time-complexity which it is $O(|V| + |E|)$.\n",
    "\n",
    "At the end of the procedure, the *fake node* and all the incident edges are deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare the input category for the example\n",
    "input_category = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the block ranking vector\n",
    "blockRanking = computeBlockRanking(input_category,category_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final node ranking algorithm follows all the steps explained in the [assignment](https://github.com/CriMenghini/ADM-2018/tree/master/Homework_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 ranked articles are the following: \n",
      "81941 82322 82082 82089 82091 81871 81878 82346 82084 81267\n",
      "The entire ranking is saved on the filesystem\n"
     ]
    }
   ],
   "source": [
    "#finally compute the nodes ranking\n",
    "compute_nodes_ranking(allGraph = final_graph, block_ranking = blockRanking, categories = category_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
