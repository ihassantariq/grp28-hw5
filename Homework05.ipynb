{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ1\n",
    "\n",
    "**Build the graph G=(V, E), where V is the set of articles and E the hyperlinks among them, and provide its basic information:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. If it is direct or not?**\n",
    "\n",
    "To answer this question we have to add edges and nodes to the networkx librarary. There is function called **nx.is_directed(G)** whether the graph is directed or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding all the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "from os.path import isfile\n",
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding functions file. \n",
    "from hw05_FUNCTIONS import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to use simple graph first to check whether it is directed or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "f = open(\"data/wiki-topcats-page-names.txt\", \"r\")\n",
    "for row in f:\n",
    "    lrow = row.split()\n",
    "    name = ' '.join(lrow[1:])\n",
    "    G.add_node(int(lrow[0]))\n",
    "    G.node[int(lrow[0])][\"name\"] = name\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'Kleroterion'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.node[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding nodes we have to add edges for these lets use **wiki-topcats-reduced.txt** file so that we have insights about the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flink = open(\"data/wiki-topcats-reduced.txt\", \"r\")\n",
    "for i,row in enumerate(flink):\n",
    "    lrow = row.split()\n",
    "    G.add_edge(int(lrow[0]), int(lrow[1]))\n",
    "flink.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_directed(G) # this function will tell us whether it is directed or not. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This tells us that graph we are trying to work with is directed because edge meaning <node1,node2> in node1 wikipedia article got a link to the node2 wikipedia article.* so this answer the question whether the graph is directed or not.* **The graph is directed**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The number of nodes? The number of edges? \n",
    "for this we are going to use simple functions like **number_of_nodes** and **number_of_edges()**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1791489"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2174451"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of nodes are **1791489** and number of edges are **2174451**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average node degree. Is the graph dense?\n",
    "\n",
    "To answer this we will use *density()** and degree function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4275348606661833\n"
     ]
    }
   ],
   "source": [
    "degreeM = 0\n",
    "for node in G.node:\n",
    "    degreeM += G.degree(node)\n",
    "print(degreeM/G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3550383037263901e-06"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.density(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to answer above question the average node degree is *2.4275348606661833* and density is *1.3550383037263901e-06*. which says that the graph is not much dense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets move to reasearch question 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RQ2:\n",
    "Given a category C_0 = \\{article_1, article_2, \\dots \\} as input we want to rank all of the nodes in V according to the given criteria discussed in given question. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we have to define constants so that we can work with our defined functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the folder path where all the data are stored\n",
    "dataFolder = \"./data/\"\n",
    "#base filename for all wiki data\n",
    "basefilename = \"wiki-topcats-\"\n",
    "#file extension of wiki data\n",
    "ext = \".txt\"\n",
    "#name of all wiki data files\n",
    "CATEGORIES = \"categories\"\n",
    "REDUCED_GRAPH = \"reduced\"\n",
    "PAGE_NAMES = \"page-names\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's open the wiki_categories.txt file and delete all categories with less than 3500 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the category file\n",
    "categories = pd.read_csv(dataFolder+basefilename+CATEGORIES+ext, sep = \";\", header = None , names = [\"Category\",\"List_of_articles\"])\n",
    "#clean the category column\n",
    "#Before: \"Category: category_name\"\n",
    "#After: \"category_name\"\n",
    "categories[\"Category\"] = categories[\"Category\"].apply(lambda x:x.split(\":\")[1])\n",
    "#defining a function that delete all categories with less than 3500 articles\n",
    "ARTICLES_THRESHOLD = 3500\n",
    "#Scanning each value of list_of_articles, if the number of articles goes beyond the threshold then retain it\n",
    "#otherwise return an empty string\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: x if len(x.strip().split(\" \")) >= ARTICLES_THRESHOLD else \"\")\n",
    "#delete all rows with empty string as \"list_of_articles\"\n",
    "categories = categories[categories[\"List_of_articles\"] != \"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is needed to build the final graph, but it must be checked that the nodes into the reduced graph and \n",
    "1. the nodes into the categories files are the same.\n",
    "2. The set of the nodes into the categories must be built.\n",
    "3. The set of nodes into the reduced graph must be built.\n",
    "4. the intersection between these two sets must be computed.\n",
    "5. only the edges involving nodes of the intersection set must be added to the final graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of set_node_1:= 428957\n",
      "size of set_node_2:= 352518\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_categories_nodes:= 546237\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_of_nodes:= 461193\n"
     ]
    }
   ],
   "source": [
    "#let's open the reduced graph file and create the set of nodes \n",
    "reduced_graph = pd.read_csv(dataFolder+basefilename+REDUCED_GRAPH+ext, sep = \"\\t\", header = None, names = [\"Node_1\",\"Node_2\"])\n",
    "#create the set of the first column\n",
    "set_node_1 = set(reduced_graph[\"Node_1\"].values.tolist())\n",
    "#create the set of the second column\n",
    "set_node_2 = set(reduced_graph[\"Node_2\"].values.tolist())\n",
    "#create the set of the nodes into the reduced graph through the union operation of the two previous created sets\n",
    "set_reduced_graph_nodes = set.union(set_node_1,set_node_2)\n",
    "#print the size for debugging\n",
    "print(\"size of set_node_1:= \"+str(len(set_node_1)))\n",
    "print(\"size of set_node_2:= \"+str(len(set_node_2)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "#The two previous sets are not useful anymore therefore they are deleted from the main memory\n",
    "del set_node_1\n",
    "del set_node_2\n",
    "\n",
    "#Let's create the set of nodes into the categories\n",
    "#create initially an empty set\n",
    "set_categories_nodes = set()\n",
    "#in order to perform the intersection function the nodes must be represented in the same format\n",
    "#since the nodes into the @reduced_graph dataframe are integer then the \"int\" type is chosen\n",
    "categories[\"List_of_articles\"].apply(lambda x: set_categories_nodes.update(set(map(int, x.strip().split(\" \")))))\n",
    "\n",
    "#compute the final set of nodes\n",
    "set_of_nodes = set.intersection(set_categories_nodes, set_reduced_graph_nodes)\n",
    "#print the size for debugging\n",
    "print(\"size of set_categories_nodes:= \"+str(len(set_categories_nodes)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "print(\"size of set_of_nodes:= \"+str(len(set_of_nodes)))\n",
    "#once the final set is computed the other sets can be deleted to free the memory\n",
    "del set_categories_nodes\n",
    "del set_reduced_graph_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is possible to read line by line all the edges of the reduced graph and adding it to the final graph only if they belong to the @set_of_nodes computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_graph = nx.DiGraph()\n",
    "\n",
    "print(\".\")\n",
    "#build the graph\n",
    "reduced_graph.apply(lambda edge: final_graph.add_edge(edge[0],edge[1]) if filterEdges(edge,set_of_nodes) else \"\" ,axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have build the graph we must filter out the nodes in the category so that we only have good nodes. For that we have used **filter_nodes_in_categories** in **hw05_FUNCTIONS.py** file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the category dataframe must involve only \"good nodes\" too\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: filter_nodes_in_categories(x,set_of_nodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we are going to use function named **retrieveCategoryDistances** for getting the categories distance. This function also uses **computeCategoryDistance** for computing the distance for each category which also internally uses **breadth_first_search** function which is being used to actually calculating the distance for each node to all other nodes. After that we will have distances for each category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_distances = retrieveCategoryDistances(filename = \"category_distances.npy\", final_graph = final_graph ,categories=categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute a dictionary that has as keys the categories and as values the list of articles that belongs to that category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dictionary = {}\n",
    "#for each category\n",
    "for i in range(categories.shape[0]):\n",
    "    #assign to that category the list of articles\n",
    "    category_dictionary[i] = list(map(int, categories.iloc[i]['List_of_articles'].split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that we will just compute the block ranking. Its just sorting of the category_distances dictionary values. We will use **computeBlockRanking** function for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockRanking = computeBlockRanking(0,category_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will compute nodes ranking using final graph, block rankings and categoris of dictionary and outputing it to the file at the same time displaying top ten nodes here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 ranked articles are the following: \n",
      "81941 82322 82082 82089 82091 81871 81878 82346 82084 81267\n",
      "The entire ranking is saved on the filesystem\n"
     ]
    }
   ],
   "source": [
    "compute_nodes_ranking(allGraph = final_graph, block_ranking = blockRanking, categories = category_dictionary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
