{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw05_FUNCTIONS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import collections\n",
    "from os.path import isfile\n",
    "from statistics import median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the folder path where all the data are stored\n",
    "dataFolder = \"./data/\"\n",
    "#base filename for all wiki data\n",
    "basefilename = \"wiki-topcats-\"\n",
    "#file extension of wiki data\n",
    "ext = \".txt\"\n",
    "#name of all wiki data files\n",
    "CATEGORIES = \"categories\"\n",
    "REDUCED_GRAPH = \"reduced\"\n",
    "PAGE_NAMES = \"page-names\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's open the wiki_categories.txt file and delete all categories\n",
    "#with less than 3500 articles\n",
    "#\n",
    "#read the category file\n",
    "categories = pd.read_csv(dataFolder+basefilename+CATEGORIES+ext, sep = \";\", header = None , names = [\"Category\",\"List_of_articles\"])\n",
    "#clean the category column\n",
    "#Before: \"Category: category_name\"\n",
    "#After: \"category_name\"\n",
    "categories[\"Category\"] = categories[\"Category\"].apply(lambda x:x.split(\":\")[1])\n",
    "#defining a function that delete all categories with less than 3500 articles\n",
    "ARTICLES_THRESHOLD = 3500\n",
    "#Scanning each value of list_of_articles, if the number of articles goes beyond the threshold then retain it\n",
    "#otherwise return an empty string\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: x if len(x.strip().split(\" \")) >= ARTICLES_THRESHOLD else \"\")\n",
    "#delete all rows with empty string as \"list_of_articles\"\n",
    "categories = categories[categories[\"List_of_articles\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of set_node_1:= 428957\n",
      "size of set_node_2:= 352518\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_categories_nodes:= 546237\n",
      "size of set_reduced_graph_nodes:= 461193\n",
      "size of set_of_nodes:= 461193\n"
     ]
    }
   ],
   "source": [
    "#Now it is needed to build the final graph, but it must be checked that the nodes into the reduced graph and \n",
    "#the nodes into the categories files are the same.\n",
    "#The set of the nodes into the categories must be built.\n",
    "#The set of nodes into the reduced graph must be built.\n",
    "#the intersection between these two sets must be computed.\n",
    "#only the edges involving nodes of the intersection set must be added to the final graph\n",
    "\n",
    "#let's open the reduced graph file and create the set of nodes \n",
    "reduced_graph = pd.read_csv(dataFolder+basefilename+REDUCED_GRAPH+ext, sep = \"\\t\", header = None, names = [\"Node_1\",\"Node_2\"])\n",
    "#create the set of the first column\n",
    "set_node_1 = set(reduced_graph[\"Node_1\"].values.tolist())\n",
    "#create the set of the second column\n",
    "set_node_2 = set(reduced_graph[\"Node_2\"].values.tolist())\n",
    "#create the set of the nodes into the reduced graph through the union operation of the two previous created sets\n",
    "set_reduced_graph_nodes = set.union(set_node_1,set_node_2)\n",
    "#print the size for debugging\n",
    "print(\"size of set_node_1:= \"+str(len(set_node_1)))\n",
    "print(\"size of set_node_2:= \"+str(len(set_node_2)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "#The two previous sets are not useful anymore therefore they are deleted from the main memory\n",
    "del set_node_1\n",
    "del set_node_2\n",
    "\n",
    "#Let's create the set of nodes into the categories\n",
    "#create initially an empty set\n",
    "set_categories_nodes = set()\n",
    "#in order to perform the intersection function the nodes must be represented in the same format\n",
    "#since the nodes into the @reduced_graph dataframe are integer then the \"int\" type is chosen\n",
    "categories[\"List_of_articles\"].apply(lambda x: set_categories_nodes.update(set(map(int, x.strip().split(\" \")))))\n",
    "\n",
    "#compute the final set of nodes\n",
    "set_of_nodes = set.intersection(set_categories_nodes, set_reduced_graph_nodes)\n",
    "#print the size for debugging\n",
    "print(\"size of set_categories_nodes:= \"+str(len(set_categories_nodes)))\n",
    "print(\"size of set_reduced_graph_nodes:= \"+str(len(set_reduced_graph_nodes)))\n",
    "print(\"size of set_of_nodes:= \"+str(len(set_of_nodes)))\n",
    "#once the final set is computed the other sets can be deleted to free the memory\n",
    "del set_categories_nodes\n",
    "del set_reduced_graph_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0          None\n",
       "1          None\n",
       "2          None\n",
       "3          None\n",
       "4          None\n",
       "5          None\n",
       "6          None\n",
       "7          None\n",
       "8          None\n",
       "9          None\n",
       "10         None\n",
       "11         None\n",
       "12         None\n",
       "13         None\n",
       "14         None\n",
       "15         None\n",
       "16         None\n",
       "17         None\n",
       "18         None\n",
       "19         None\n",
       "20         None\n",
       "21         None\n",
       "22         None\n",
       "23         None\n",
       "24         None\n",
       "25         None\n",
       "26         None\n",
       "27         None\n",
       "28         None\n",
       "29         None\n",
       "           ... \n",
       "2645217    None\n",
       "2645218    None\n",
       "2645219    None\n",
       "2645220    None\n",
       "2645221    None\n",
       "2645222    None\n",
       "2645223    None\n",
       "2645224    None\n",
       "2645225    None\n",
       "2645226    None\n",
       "2645227    None\n",
       "2645228    None\n",
       "2645229    None\n",
       "2645230    None\n",
       "2645231    None\n",
       "2645232    None\n",
       "2645233    None\n",
       "2645234    None\n",
       "2645235    None\n",
       "2645236    None\n",
       "2645237    None\n",
       "2645238    None\n",
       "2645239    None\n",
       "2645240    None\n",
       "2645241    None\n",
       "2645242    None\n",
       "2645243    None\n",
       "2645244    None\n",
       "2645245    None\n",
       "2645246    None\n",
       "Length: 2645247, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now it is possible to read line by line all the edges of the reduced graph\n",
    "#and adding it to the final graph only if they belong to the @set_of_nodes computed\n",
    "final_graph = nx.DiGraph()\n",
    "\n",
    "print(\".\")\n",
    "#build the graph\n",
    "reduced_graph.apply(lambda edge: final_graph.add_edge(edge[0],edge[1]) if filterEdges(edge,set_of_nodes) else \"\" ,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the category dataframe must involve only \"good nodes\" too\n",
    "categories[\"List_of_articles\"] = categories[\"List_of_articles\"].apply(lambda x: filter_nodes_in_categories(x,set_of_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_distances = retrieveCategoryDistances(filename = \"category_distances.npy\", final_graph = final_graph ,categories=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute a dictionary that has as keys the categories and as values the list of articles\n",
    "#that belongs to that category\n",
    "category_dictionary = {}\n",
    "#for each category\n",
    "for i in range(categories.shape[0]):\n",
    "    #assign to that category the list of articles\n",
    "    category_dictionary[i] = list(map(int, categories.iloc[i]['List_of_articles'].split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockRanking = computeBlockRanking(0,category_distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top-10 ranked articles are the following: \n",
      "81941 82322 82082 82089 82091 81871 81878 82346 82084 81267\n",
      "The entire ranking is saved on the filesystem\n"
     ]
    }
   ],
   "source": [
    "compute_nodes_ranking(allGraph = final_graph, block_ranking = blockRanking, categories = category_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
